{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright UCL Business plc 2017. Patent Pending. All rights reserved.\n",
    "#\n",
    "# The MonoDepth Software is licensed under the terms of the UCLB ACP-A licence\n",
    "# which allows for non-commercial use only, the full terms of which are made\n",
    "# available in the LICENSE file.\n",
    "#\n",
    "# \n",
    "# Modified by Yongyang Nie.\n",
    "# Copyright (c) by Yongyang Nie \n",
    "# All Rights Reserved\n",
    "# Contact: contact@neilnie.com\n",
    "#\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "# only keep warnings and errors\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from monodepth_model import *\n",
    "from monodepth_dataloader import *\n",
    "from average_gradients import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_disparity(disp):\n",
    "    _, h, w = disp.shape\n",
    "    l_disp = disp[0,:,:]\n",
    "    r_disp = np.fliplr(disp[1,:,:])\n",
    "    m_disp = 0.5 * (l_disp + r_disp)\n",
    "    l, _ = np.meshgrid(np.linspace(0, 1, w), np.linspace(0, 1, h))\n",
    "    l_mask = 1.0 - np.clip(20 * (l - 0.05), 0, 1)\n",
    "    r_mask = np.fliplr(l_mask)\n",
    "\n",
    "    return r_mask * l_disp + l_mask * r_disp + (1.0 - l_mask - r_mask) * m_disp\n",
    "\n",
    "# github.com/aubricus\n",
    "def print_progress(iteration, total, prefix='', suffix='', decimals=1, bar_length=100):\n",
    "    \"\"\"\n",
    "    Call in a loop to create terminal progress bar\n",
    "    @params:\n",
    "        iteration   - Required  : current iteration (Int)\n",
    "        total       - Required  : total iterations (Int)\n",
    "        prefix      - Optional  : prefix string (Str)\n",
    "        suffix      - Optional  : suffix string (Str)\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "        bar_length  - Optional  : character length of bar (Int)\n",
    "    \"\"\"\n",
    "    str_format = \"{0:.\" + str(decimals) + \"f}\"\n",
    "    percents = str_format.format(100 * (iteration / float(total)))\n",
    "    filled_length = int(round(bar_length * iteration / float(total)))\n",
    "    bar = '█' * filled_length + '-' * (bar_length - filled_length)\n",
    "\n",
    "    sys.stdout.write('\\r%s |%s| %s%s %s' % (prefix, bar, percents, '%', suffix)),\n",
    "\n",
    "    if iteration == total:\n",
    "        sys.stdout.write('\\n')\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = '/home/neil/Workspace/monodepth/models/cityscape/model_cityscapes.data-00000-of-00001'\n",
    "input_height = 256\n",
    "input_width =512 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/neil/Workspace/monodepth/models/cityscape/model_cityscapes\n"
     ]
    }
   ],
   "source": [
    "params = monodepth_parameters(\n",
    "        encoder='vgg',\n",
    "        height=input_height,\n",
    "        width=input_width,\n",
    "        batch_size=2,\n",
    "        num_threads=1,\n",
    "        num_epochs=1,\n",
    "        do_stereo=False,\n",
    "        wrap_mode=\"border\",\n",
    "        use_deconv=False,\n",
    "        alpha_image_loss=0,\n",
    "        disp_gradient_loss_weight=0,\n",
    "        lr_loss_weight=0,\n",
    "        full_summary=False)\n",
    "\n",
    "left  = tf.placeholder(tf.float32, [2, input_height, input_width, 3])\n",
    "model = MonodepthModel(params, \"test\", left, None)\n",
    "\n",
    "# SESSION\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "# SAVER\n",
    "train_saver = tf.train.Saver()\n",
    "\n",
    "# INIT\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())\n",
    "\n",
    "# RESTORE\n",
    "restore_path = checkpoint_path.split(\".\")[0]\n",
    "train_saver.restore(sess, restore_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new directory for depth images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making new directories... \n",
      "['krefeld', 'zurich', 'monchengladbach', 'freiburg', 'augsburg', 'duisburg', 'muhlheim-ruhr', 'schweinfurt', 'bad-honnef', 'nuremberg', 'darmstadt', 'ulm', 'konstanz', 'hamburg', 'saarbrucken', 'wuppertal', 'wurzburg', 'bayreuth', 'bochum', 'erfurt', 'bremen', 'stuttgart', 'dusseldorf', 'mannheim', 'jena', 'konigswinter', 'erlangen', 'troisdorf', 'strasbourg', 'heilbronn', 'hanover', 'cologne', 'dortmund', 'heidelberg', 'oberhausen', 'bamberg', 'karlsruhe', 'dresden', 'tubingen', 'weimar', 'aachen']\n",
      "directory: krefeld already exists\n",
      "directory: zurich already exists\n",
      "directory: monchengladbach already exists\n",
      "directory: freiburg already exists\n",
      "directory: augsburg already exists\n",
      "directory: duisburg already exists\n",
      "directory: muhlheim-ruhr already exists\n",
      "directory: schweinfurt already exists\n",
      "directory: bad-honnef already exists\n",
      "directory: nuremberg already exists\n",
      "directory: darmstadt already exists\n",
      "directory: ulm already exists\n",
      "directory: konstanz already exists\n",
      "directory: hamburg already exists\n",
      "directory: saarbrucken already exists\n",
      "directory: wuppertal already exists\n",
      "directory: wurzburg already exists\n",
      "directory: bayreuth already exists\n",
      "directory: bochum already exists\n",
      "directory: erfurt already exists\n",
      "directory: bremen already exists\n",
      "directory: stuttgart already exists\n",
      "directory: dusseldorf already exists\n",
      "directory: mannheim already exists\n",
      "directory: jena already exists\n",
      "directory: konigswinter already exists\n",
      "directory: erlangen already exists\n",
      "directory: troisdorf already exists\n",
      "directory: strasbourg already exists\n",
      "directory: heilbronn already exists\n",
      "directory: hanover already exists\n",
      "directory: cologne already exists\n",
      "directory: dortmund already exists\n",
      "directory: heidelberg already exists\n",
      "directory: oberhausen already exists\n",
      "directory: bamberg already exists\n",
      "directory: karlsruhe already exists\n",
      "directory: dresden already exists\n",
      "directory: tubingen already exists\n",
      "directory: weimar already exists\n",
      "directory: aachen already exists\n"
     ]
    }
   ],
   "source": [
    "dirs = os.listdir('/hdd/ssd_2/dataset/segmentation/training')\n",
    "print(\"making new directories... \\n\" + str(dirs))\n",
    "\n",
    "for d in dirs:\n",
    "    if (os.path.isdir('/hdd/ssd_2/dataset/segmentation/training_depth/' + d)):\n",
    "        print(\"directory: \" + d + \" already exists\")\n",
    "    else:\n",
    "        os.mkdir('/hdd/ssd_2/dataset/segmentation/training_depth/' + d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop through all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  if sys.path[0] == '':\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:21: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |█-------------------------------------------------| 2.5% Complete"
     ]
    }
   ],
   "source": [
    "labels = pd.read_csv('/hdd/ssd_2/dataset/segmentation/labels.csv').values\n",
    "i = 0\n",
    "j = 0\n",
    "\n",
    "for row in labels:\n",
    "    \n",
    "    if i > 6416:\n",
    "        image_path = row[0]\n",
    "    \n",
    "        input_image = scipy.misc.imread(image_path, mode=\"RGB\")\n",
    "        original_height, original_width, num_channels = input_image.shape\n",
    "        input_image = scipy.misc.imresize(input_image, [input_height, input_width], interp='lanczos')\n",
    "        input_image = input_image.astype(np.float32) / 255\n",
    "        input_images = np.stack((input_image, np.fliplr(input_image)), 0)\n",
    "\n",
    "        disp = sess.run(model.disp_left_est[0], feed_dict={left: input_images})\n",
    "        disp_pp = post_process_disparity(disp.squeeze()).astype(np.float32)\n",
    "    \n",
    "        new_image_path = image_path.replace(\"training\", \"training_depth\")\n",
    "\n",
    "        disp_to_img = scipy.misc.imresize(disp_pp.squeeze(), [original_height, original_width])\n",
    "        plt.imsave(new_image_path, disp_to_img, cmap='gray')\n",
    "        j = j + 1\n",
    "        print_progress(j, 16557, prefix='Progress:', suffix='Complete', bar_length = 50)\n",
    "    \n",
    "    i = i + 1\n",
    "        \n",
    "print(\"data finished processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
